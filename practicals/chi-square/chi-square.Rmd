---
title: "The Chi-square distribution and related tests"
author: "Quentin Ferr√© & Denis Puthier"
date: '`r Sys.Date()`'
output:
  html_document:
    fig_caption: yes
    highlight: zenburn
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float: yes
    collapsed: false
    smooth_scroll: false
  pdf_document:
    fig_caption: yes
    highlight: zenburn
    toc: no
    toc_depth: 3
  word_document: default
css: course.css
---


<script type="">
    $(function() {
        $(".hideshow").click(function() {
                $(this).parent().find(".exo_code").toggle();
        });
            
    })
</script>

<style>
.exo_code {
  display:none;
}

pre  {
  background-color: #2C3539!important;
  color: white;
} 

pre.text {
  background-color: #2C3539 !important;
}

hr {
  color: grey !important;
}


hr {
  color: grey !important;
}
</style>



# The 	$\chi^2$ distribution

## Definition

**from wikipedia**: In probability theory and statistics, the chi-square distribution (also chi-square or $\chi^2$-distribution) with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables.

In other words, given  $x_1, x_2, ... , x_i$, $k$ independent variables drawn from a normal distribution  with mean 0 and standard deviation1, then the sum of their squares:

$$Q\ = \sum_{i=1}^k x_i^2$$

is distributed according to the chi-square distribution with $k$ degrees of freedom. It is generally denoted as 
$$Q\ \sim\ \chi^2(k)\ \ \text{or}\ \ Q\ \sim\ \chi^2_k .$$

## Probability density function of $\chi^2$ distribution 

The probability density function (pdf) of the chi-square distribution is:

$$f(x;\,k) =
\begin{cases}
  \dfrac{x^{\frac k 2 -1} e^{-\frac x 2}}{2^{\frac k 2} \Gamma\left(\frac k 2 \right)},  & x > 0; \\ 0, & \text{otherwise}.
\end{cases}$$


<div class="exo">
- Check, using a diagram and using standard R functions/operators that, for $k_val=5$ the mathematical formula fits with the values returns by *dchisq()* function.
</div>
<div class="hideshow"> << Hide | Show >> </div>
<div class="exo_code">
```{r}
k_val <- 5
x <- 0:30
denom = (2^(k_val/2)*gamma(k_val/2)) 
num = x^(k_val/2-1)*exp(-x/2)

plot(0:30, num/denom, 
     type="l",
     col="blue",
     panel.first = grid(),
     xlab="x",
     ylab="f(x;k)")
points(0:30,
       dchisq(0:30, df=k_val),
       col="red",
       lw=2,
       pch=16)
```
</div>


## Testing the model


In the exercice below will compare the $\chi^2$ distribution model to a distribution obtained by randomly computing values for $Q$ with a fixed degree of freedom. 

**NB**: Regarding R code, we will use "_" characters for naming variables instead of classical '.'. This refers to the official [Hadley style of coding](http://stat405.had.co.nz/r-style.html). This is more Pythonic !!



<div class="exo">
- create a variable $k\_val$ that will be set to 5.
- Using the *replicate()* function and the *rnorm()* function, create 100000 samples of size $k\_val$ taken from a normal distribution with population mean 0 and standard deviation 1.
- Using *colSums()*, compute, for each sample, the sum of the squares.
- Draw the distribution of the obtained dataset.
    - Using the *hist()* function.
    - Using the *density()* and *plot()* function.
- Using the *dchisq()* function, check that the obtained distribution is well modeled by the $\chi^2$ distribution.
- Compute the $\chi^2$ distribution using $k\_val$ ranging from 5 to 50 with step by 5. 
- Conclude.
</div>
<div class="hideshow"> << Hide | Show >> </div>
<div class="exo_code">
```{r}
#-----------------------------------------------------------------------
# Using the replicate() function and the rnorm() function, create 100000 
# samples of size k_val taken from a 
# normal distribution with population mean 0 and standard deviation 1
#-----------------------------------------------------------------------
n <- 100000
x_val <- replicate(n, rnorm(5, mean = 0, sd = 1))

#-----------------------------------------------------------------------
# Using *colSums()*, compute, for each sample, the sum of the squares.
#-----------------------------------------------------------------------
x_srqt_sum <- colSums(x_val^2)
par(mfrow=c(1,2))

#-----------------------------------------------------------------------
# Draw the distribution of the obtained dataset.
#-----------------------------------------------------------------------

hist(x_srqt_sum, 
     col="blue", 
     border = "white", 
     breaks = 100)
plot(density(x_srqt_sum), 
     lwd=2, 
     col="blue", 
     panel.first=grid(),
     xlab="x")

#-----------------------------------------------------------------------
# Using the *dchisq* function, check that the obtained distribution is 
# well modeled by the $\chi^2$ distribution.
#-----------------------------------------------------------------------
par(mfrow=c(1,1))
plot(density(x_srqt_sum), 
     lwd=2, 
     col="blue", 
     panel.first=grid(),
     xlab="x")
points(0:30,
       dchisq(0:30, df=k_val),
       col="red",
       lw=2,
       pch=16)

```

</div>



## Computing the $\chi^2$ distribution for various degree of freedom

TODO: intro

<div class="exo">
- Compute the $\chi^2$ distribution for various degree of freedom (1,5,15...50).
</div>
<div class="hideshow"> << Hide | Show >> </div>
<div class="exo_code">
```{r}

## Create a color palette
if(!require(RColorBrewer)) install.packages('RColorBrewer')
color_pal <- brewer.pal(11, name = "Paired")


## Compute and draw density diagram of Chi-square Distribution
# The values taken by k will be successively 1,5,15...50
k_val <- c(1, seq(5, 50, by=5))

# Draw a Chi-square Distribution for k = 1
xlim <- c(0,100)
i = 1
plot(seq(xlim[1], xlim[2], length.out = 100), 
     dchisq(k_val[i], x=seq(xlim[1], xlim[2], length.out = 100)), 
     type="l", col= color_pal[i], 
     panel.first = grid(),
     lwd=2,
     xlab="x",
     ylab="f(x;k)"
     )

for(i in 2:length(k_val)){
  points(seq(xlim[1], xlim[2], length.out = 100), 
         dchisq(k_val[i], x=seq(xlim[1], xlim[2], length.out = 100)), 
         type="l", 
         col= color_pal[i],
         lwd=2)
}

legend("topright", 
       legend=paste("k=", k_val, sep=""), 
       fill=color_pal)
```
</div>


# Chi-square test

In statistics, a $\chi^2$ test, is a statistical test in which the sampling distribution of the test statistic follows a $\chi^2$ distribution under the null hypothesis. The $\chi^2$ statistics is used in several statistical tests. However, without other qualification, 'chi-square test' is often used as short for Pearson's chi-square test. The chi-square test is used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categories. 


## Pearson's chi-square test

Suppose that n observations in a random sample from a population are classified into k mutually exclusive classes with respective observed numbers $x_i$ (for $i=1,2...k$),and a null hypothesis gives the probability $p_i$ that an observation falls into the ith class. So we have the expected numbers $m_i = np_i$ for all $i$, where:

$$\sum^k_{i=1}{p_i} = 1$$
$$\sum^k_{i=1}{m_i} =n\sum^k_{i=1}{p_i} = \sum_{i=1}^{k}x_i$$

Pearson proposed that, under the circumstance of the null hypothesis being correct, the score below follows $\chi^2$ distribution with $k-1$ degrees of freedom.


$$X^2=\sum^k_{i=1}{\frac{(x_i-m_i)^2}{m_i}}=\sum^k_{i=1}{\frac{x_i^2}{m_i}-n}$$

This score is frequently viewed as:

$$X^2=\sum^k_{i=1}{\frac{(observed_i-expected_i)^2}{expected_i}}$$


### Using chi-square to test for goodness of fit


The Pearson's chi-square test is frequently used to test whether a random variable $Y$ has a distribution that is compatible (*i.e* can be well modeled) with a known probability distribution (*e.g* normal, geometric, binomial...). It is thus a measure of, the goodness of fit, that is, how well the  model fits the variable $Y$. When testing for goodness of fit using a $chi^2$ test, the null hypothesis is thus "$Y$ follows a distribution P".


#### Example: checking the balance of a gaming dice

A gambler plays a game that involves throwing a dice in a succession of 100 trials. If the die being used is fair, then the chance of any particular number coming up is the same: 1/6. However, if the gambler uses a loaded diethen certain numbers will have a greater likelihood of appearing, while others will have a lower likelihood. The fairness of the dice can be tested using a $\chi^2$ test.

Let's consider a random variable $Y$ that corresponds to the results obtained after throwing a dice 100 times:

```{r}
y <- c(11, 15, 16, 17, 18, 23)
names(y) <- 1:6
```

<div class="exo">
- Create a vector containing the probability associated with each face of a fair dice. 
- Create a vector containing the expected number of success for each face with a fair dice.
- Compute the $\chi^2$ statistics.
- Compute the p-value for the associated $\chi^2$ statistics (the degree of freedom is $k-1$).   
- Check that the same p-value is obtained using *chisq.test()*.
- Conclusion ?
</div>
<div class="hideshow"> << Hide | Show >> </div>
<div class="exo_code">
```{r}
## Create a vector containing the probability associated with each face of a fair dice. 
prob <- rep(1/6, 6)

## Create a vector containing the expected number of success for each face with a fair dice. 
expected <- prob * 100

## Compute the $\chi^2$ statistics
chi_stat <- sum((y - expected)^2/expected)

## Compute the p-value for the associated $\chi^2$ statistics
pchisq(q=chi_stat, df=length(y)-1, lower.tail = FALSE)

## Check that the same p-value is obtained using chisq.test()
chi_test_res <- chisq.test(x=y, p=prob)
print(chi_test_res)
is(chi_test_res)
names(chi_test_res)
chi_test_res$p.value

## Conclude
# The observed sample frequencies do not differ significantly 
# from the expected frequencies of a fair dice. 
```
</div>

#### Example: testing for a Poisson distribution.


Let's consider a random variable $Y$ with null or positive discrete values corresponding to the number of vehicule arriving at one-minute interval at a given point and recorded during $n=109$ minutes. The objective is to test whether this sample can be properly modeled using a Poisson distribution. 

<div class="tips">
**Recall**: The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. The  expected value and variance of a Poisson-distributed random variable is $\lambda$. Its probability mass function is provided by the formula below:

$P(X=x) = \frac{\lambda^xe^-\lambda}{x!}$

</div>


Copy and paste the code below to create the variable $Y$.

```{r}
y_obs <- c(0, 0, 1, 4, 5, 6, 11, 14, 13, 10, 8,10, 7, 4, 6, 5, 4, 0, 1)
names(y_obs) <- 0:18
sum(y_obs)
```

<div class="attention">
**NB:** It is common that the parameters from the hypothesized distribution are estimated from the dataset itself. In this case we will use $k-r-1$ degree of freedom where $r$ corresponds to the number of parameters in the hypothesized distribution to be estimated.
</div>

Here, the $lambda$ parameters from the Poisson model will be estimated from the sample mean:

```{r}
mu <- 1/sum(y_obs)*sum(0:(length(y_obs)-1)*y_obs)
(lambda <- mu)
```

<div class="attention">
**NB:** In general, a Chi-square test is appropriate when the expected values are "sufficiently" large. The classical threshold for this condition of applicability is to require exp(i) >= 5 for each class. As a rule if one of the class contains less than 5 observation, combine it with a neighboring class. In our case we will combine classes in the following way:
</div>

```{r}
y_merged <- c(sum(y_obs[1:5]), 6, 14, 11, 8, 10, 13,10, 7, 4, 6, sum(y_obs[16:19]))
names(y_merged) <- c("0-4", as.character(5:14), ">14")
```



<div class="exo">
- Draw the histogram of P(X=x) for x in $0,1...18$ under a Poisson model of $\lambda=\mu$. Use the *dpois()* and  *plot()* function (type="h" for the later). 
- Draw the histogram of the expected number of counts for x in $0,1...18$.
- Draw the histogram of the observed values.
- What is the probability associated to each class under a Poisson model with $\lambda=\mu$. Use de *dpois()* function ? Check sum of probabilities equal to 1.
- Compute the $\chi^2$ statistics.
- Compute the p-value for the associated $\chi^2$ statistics. 
- Conclude.
</div>
<div class="hideshow"> << Hide | Show >> </div>
<div class="exo_code">
```{r}

## Draw the histogram of P(X=x) for x in $0,1...18$ under a Poisson model of lambda=mu
plot(x=0:18, 
     y=dpois(0:18, lambda = mu), 
     type="h", 
     col="blue", 
     lwd=3,
     panel.first = grid())
## Draw the histogram of the expected number of counts for x in 0,1...18
plot(x=0:18, 
     y=dpois(0:18, lambda = mu)*sum(y_obs), 
     type="h", 
     col="blue", 
     lwd=3,
     panel.first = grid())
## Draw the histogram of the observed values
plot(x=(0:18)-0.1, 
     y=dpois(0:18, lambda = mu)*sum(y_obs), 
     type="h", 
     col="blue", 
     lwd=3,
     panel.first = grid())
points(x=(0:18)+0.1, 
     y=y_obs, 
     type="h", 
     col="red", 
     lwd=3,
     panel.first = grid())
## What is the probability associated to each class under a Poisson model with
prob <- c(sum(dpois(0:4, lambda = mu)), dpois(5:14, lambda = mu), 1- sum(dpois(0:14, lambda=mu)))

## Compute the chi-square statistics.
expected <- prob*sum(y_obs)
sum(expected) == sum(y_obs)
chi_stat <- sum((y_merged-expected)^2/expected)

## Compute the p-value for the associated chi-square statistics.
pchisq(q=chi_stat, df=length(y_merged)-1-1, lower.tail = FALSE)

## Conclude
# The observed sample frequencies do not differ significantly 
# from the expected frequencies of a Poisson model with lambda = mu
```

</div>

### Chi-square Test of Independence

The Chi-Square test of independence is used to determine if there is a significant relationship between two categorical variables. It can be used, for instance, to check whether age is related to voting preference, whether gender is related to sport preference...

The null hypothesis of the chi-square Test of Independence is :

$H_0:  variables\ are\ independent.$

If the null hypothesis is rejected, we accept the alternative hypothesis:

$H_1:  variables\ are\ not\ independent.$

#### Placebo vs treatment example

In the context of Neuroblastoma treatment, clinicians have tested a new drug, $D$, which has been shown to reduce tumor size both *in vitro* and model animals. The current study was conduct on 546 individuals diagnosed with Neuroblastoma and that did not responde to classical chemotherapy line. The results are provided below.


```{r}
library(knitr)
res <- data.frame(Tumor_reduction = c(178, 145), No_tumor_reduction = c(110, 136))
rownames(res) <- c("Treatment","Placebo")
kable(res)
```

<div class="warning">
In the case of a contingency table the degree of freedom of the $chi^2$ statistic is compute using:
$df = (r-1)(c-1)$
where $r$ is the number of rows and $c$ is the number of columns.
<div>

<div class="exo">
- Check using the *chisq.text()* function that an association exists between treatment and tumor size reduction.
- Check that the same result is obtained using $pchisq()$.
- Compute the Odd ratio.
</div>

```{r}
## Check using the *chisq.text()* function that an association exists between treatment
res_test <- chisq.test(res, correct=FALSE)
print(res_test$p.value)

##  Check that the same result is obtained using pchisq()
# or t(outer(colSums(res)/sum(colSums(res)), 
# rev(rowSums(res))))[rownames(res),]
row_sum <- rowSums(res)
colSum <- colSums(res)
expected <- rbind(colSum/sum(colSum) * row_sum[1], 
                  colSum/sum(colSum) * row_sum[2])
pval <- pchisq(sum((res-expected)^2/expected), df = 1, lower.tail = FALSE)
all.equal(pval, res_test$p.value)

## Odd ratio (OR)
# OR > 1 thus
# Tumor reduction is more frequent in patient treated with drug D compared
# to placebo-treated patients
p1 <- res[1,1]/(res[1,1] + res[1,2])
p2 <- res[2,1]/(res[2,1] + res[2,2])
(p1/(1-p1)) / (p2/(1-p2))
```


#### Alternative promoters example

We work in lymphoctes here.

Genes can have multiple promoters. In this example, we perform a ChIP-Seq of H3K4me3 marks against several genes. Here, we provide a table of peak coverage (in thousands of reads) for each H3K4me3 peak that intersect a known TSS for the given genes.

This ChIP-Seq is run in three different conditions : healthy thymic cells (normal), leukemic thymic cells (t_all) and immortalized cell lines (cell_line).

The question we wish to answer is whether alternative promoter usage correlates to the cell status (healthy, cancerous).

```{r altprom}
altprom = read.csv('https://raw.githubusercontent.com/dputhier/SAOD---Statistical-analysis-of-omics-data/gh-pages/practicals/chi-square/alt_prom_coverage.tsv', sep='\t')


chisq.results = list()
for (gene in unique(altprom$gene)){
  table =  altprom[altprom$gene == gene,2:4]
  chisq.results[gene] = chisq.test(table)
}
```


*Your goal* : run the Chi-Squared test for all conditions, per gene.

You will take notes of any error messages and try to interpret the results.


